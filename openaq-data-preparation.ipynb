{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "import pandas as pd\r\n",
    "import datetime\r\n",
    "import os\r\n",
    "import boto3\r\n",
    "\r\n",
    "from pyspark import SparkContext, SparkConf\r\n",
    "from pyspark.sql import SparkSession\r\n",
    "\r\n",
    "import sagemaker\r\n",
    "from sagemaker import get_execution_role\r\n",
    "import sagemaker_pyspark\r\n",
    "from datetime import datetime, timedelta\r\n",
    "from pyspark.sql.functions import to_date,date_format,col,from_unixtime,unix_timestamp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "role = get_execution_role()\r\n",
    "jars = sagemaker_pyspark.classpath_jars()\r\n",
    "\r\n",
    "classpath = \":\".join(sagemaker_pyspark.classpath_jars())\r\n",
    "spark = (\r\n",
    "    SparkSession.builder.config(\"spark.driver.extraClassPath\", classpath)\r\n",
    "    .master(\"local[*]\")\r\n",
    "    .getOrCreate()\r\n",
    ")\r\n",
    "\r\n",
    "spark"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f1d91052358>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-16-93-168.ec2.internal:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "from pyspark.sql.functions import lit\r\n",
    "\r\n",
    "region = boto3.Session().region_name\r\n",
    "endpoint_domain = \"com\"\r\n",
    "spark._jsc.hadoopConfiguration().set(\r\n",
    "    \"fs.s3a.endpoint\", \"s3.{}.amazonaws.{}\".format(region, endpoint_domain)\r\n",
    ")\r\n",
    "\r\n",
    "first_date = \"2022-01-01\"\r\n",
    "\r\n",
    "start_date = datetime.strptime('2022-01-02', '%Y-%m-%d') \r\n",
    "end_date = datetime.utcnow()\r\n",
    "\r\n",
    "delta = end_date - start_date\r\n",
    "\r\n",
    "day = \"\"\r\n",
    "df = (\r\n",
    "        spark.read.format(\"json\")\r\n",
    "        .option(\"numFeatures\", \"784\")\r\n",
    "        .load(f\"s3a://openaq-fetches/realtime-gzipped/{first_date}\".format(region))\r\n",
    "    )\r\n",
    "df = df.withColumn(\"u_date\",lit(first_date)).withColumn(\"lat\",col(\"coordinates.latitude\")).withColumn(\"long\",col(\"coordinates.longitude\"))\r\n",
    "\r\n",
    "for i in range(delta.days + 1):\r\n",
    "    day = start_date + timedelta(days=i)\r\n",
    "    day = day.strftime(\"%Y-%m-%d\")\r\n",
    "\r\n",
    "    temp_df = (\r\n",
    "        spark.read.format(\"json\")\r\n",
    "        .option(\"numFeatures\", \"784\")\r\n",
    "        .load(f\"s3a://openaq-fetches/realtime-gzipped/{day}\".format(region))\r\n",
    "    )\r\n",
    "    temp_df = temp_df.withColumn(\"u_date\",lit(day)).withColumn(\"lat\",col(\"coordinates.latitude\")).withColumn(\"long\",col(\"coordinates.longitude\"))\r\n",
    "\r\n",
    "    diff1 = [c for c in temp_df.columns if c not in df.columns]\r\n",
    "    diff2 = [c for c in df.columns if c not in temp_df.columns]\r\n",
    "    df = df.select('*', *[lit(None).alias(c) for c in diff1]) \\\r\n",
    "    .unionByName(temp_df.select('*', *[lit(None).alias(c) for c in diff2]))\r\n",
    "\r\n",
    "\r\n",
    "df.show()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "df1 = df.filter(df.country == \"US\")\r\n",
    "df1 = df1.select(['city','country','lat','long','u_date','location','parameter','unit','value'])\r\n",
    "df1.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------------+-------+---------+-----------+----------+--------------------+---------+-----+-----+\n",
      "|             city|country|      lat|       long|    u_date|            location|parameter| unit|value|\n",
      "+-----------------+-------+---------+-----------+----------+--------------------+---------+-----+-----+\n",
      "|Birmingham-Hoover|     US|  33.5531|    -86.815|2022-01-01|            NO. BHAM|       o3|  ppm|0.021|\n",
      "|Birmingham-Hoover|     US|  33.5531|    -86.815|2022-01-01|            NO. BHAM|     pm10|µg/m³| 16.0|\n",
      "|Birmingham-Hoover|     US|  33.5531|    -86.815|2022-01-01|            NO. BHAM|     pm25|µg/m³|  8.0|\n",
      "|Birmingham-Hoover|     US|  33.3311|   -87.0036|2022-01-01|             MCADORY|     pm25|µg/m³|  7.4|\n",
      "|Birmingham-Hoover|     US|  33.4997|   -86.9242|2022-01-01|               WYLAM|     pm10|µg/m³| 17.0|\n",
      "|Birmingham-Hoover|     US|  33.4997|   -86.9242|2022-01-01|               WYLAM|     pm25|µg/m³|  8.6|\n",
      "|Birmingham-Hoover|     US|  33.8017|   -86.9425|2022-01-01|              CORNER|     pm25|µg/m³|  6.5|\n",
      "|Birmingham-Hoover|     US|33.565278| -86.796389|2022-01-01|       Shuttlesworth|     pm10|µg/m³| 15.0|\n",
      "|Birmingham-Hoover|     US|33.565278| -86.796389|2022-01-01|       Shuttlesworth|     pm25|µg/m³|  9.4|\n",
      "|       Huntsville|     US|  34.6917| -86.591698|2022-01-01|Huntsville Old Airpo|     pm25|µg/m³|  7.9|\n",
      "|       Montgomery|     US|32.412811| -86.263394|2022-01-01|               MONTG|     pm25|µg/m³| 11.7|\n",
      "|          Decatur|     US|  34.5286|   -86.9706|2022-01-01|             DECATUR|     pm25|µg/m³|  7.7|\n",
      "|           SUMTER|     US| 32.36253|  -88.27792|2022-01-01|                Ward|     pm25|µg/m³|  2.6|\n",
      "|        Anchorage|     US|61.205861|  -149.8246|2022-01-01|              Garden|     pm10|µg/m³|  2.0|\n",
      "|        Anchorage|     US|61.205861|  -149.8246|2022-01-01|              Garden|     pm25|µg/m³|  6.0|\n",
      "|        Anchorage|     US|  61.3267|-149.569707|2022-01-01|Parkgate (Eagle Rive|     pm10|µg/m³|  7.0|\n",
      "|    YUKON-KOYUKUK|     US|63.723301|-148.967499|2022-01-01|          Denali NPP|       o3|  ppm|0.036|\n",
      "|        Fairbanks|     US|  64.8458| -147.72727|2022-01-01|               NCore|       co|  ppm| 0.16|\n",
      "|        Fairbanks|     US|  64.8458| -147.72727|2022-01-01|               NCore|       o3|  ppm| 0.03|\n",
      "|        Fairbanks|     US|  64.8458| -147.72727|2022-01-01|               NCore|     pm25|µg/m³|  6.0|\n",
      "+-----------------+-------+---------+-----------+----------+--------------------+---------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "df2 = df1.withColumn(\"month\",from_unixtime(unix_timestamp(col(\"u_date\"),'yyyy-MM-DD'),'MMM'))\r\n",
    "df3 = df2.groupBy([\"city\",\"month\"]).agg({'value':'avg'})\r\n",
    "df3.count()\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1230"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df3.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"s3a://openaq-analytics/2022/\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}